% \documentclass[sigconf,anonymous,review]{acmart}
\documentclass[sigconf]{acmart}
\acmSubmissionID{1234}

\usepackage{booktabs} % For formal tables

% TOG prefers author-name bib system with square brackets
\citestyle{acmauthoryear}
%\setcitestyle{nosort,square} % nosort to allow for manual chronological ordering

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}

\usepackage[acronyms]{glossaries}

\newcommand{\citeColored}[1]{\textcolor{blue}{\cite{#1}}}
\newcommand{\toDo}[1]{\textcolor{red}{#1}}

% Metadata Information
% \copyrightyear{2022}
% \acmYear{2022}
% \setcopyright{acmcopyright}
% \acmConference{Conference Name}{Conference Date and Year}{Conference Location}
% \acmDOI{10.1145/8888888.7777777}
% \acmISBN{978-1-4503-1234-5/22/07}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\acmDOI{0000001.0000001_2}

% Paper history
%\received{February 2007}
%\received{March 2009}
%\received[final version]{June 2009}
%\received[accepted]{July 2009}

% Document starts
\begin{document}
% Title portion
\title{DistillNeRF: Data-Efficient Breeding of Neural Radiance Fields using Knowledge Distillation}

% DO NOT ENTER AUTHOR INFORMATION FOR ANONYMOUS TECHNICAL PAPER SUBMISSIONS TO SIGGRAPH 2019!
\author{Yliess Hati}
\affiliation{
    \institution{Pôle Universitaire Leonard de Vinci, Research Center}
    \streetaddress{12 Av. Léonard de Vinci}
    \city{Courbevoie}
    \postcode{92400}
    \country{France}
}
\affiliation{
    \institution{Université de Reims Champagne-Ardenne, CReSTIC}
    \streetaddress{UFR Sciences Exactes et Naturelles, Moulin de la Housse}
    \city{Reims}
    \postcode{51867}
    \country{France}
}
\email{yliess.hati@devinci.fr}

\author{Florent Nolot}
\affiliation{
    \institution{Université de Reims Champagne-Ardenne, CReSTIC}
    \streetaddress{UFR Sciences Exactes et Naturelles, Moulin de la Housse}
    \city{Reims}
    \postcode{51867}
    \country{France}
}
\email{florent.nolot@univ-reims.fr}

\author{Francis Rousseaux}
\affiliation{
    \institution{Université de Reims Champagne-Ardenne, CReSTIC}
    \streetaddress{UFR Sciences Exactes et Naturelles, Moulin de la Housse}
    \city{Reims}
    \postcode{51867}
    \country{France}
}
\email{francis.rousseaux@univ-reims.fr}
    
\author{Clément Duhart}
\affiliation{
    \institution{Pôle Universitaire Leonard de Vinci, Research Center}
    \streetaddress{12 Av. Léonard de Vinci}
    \city{Courbevoie}
    \postcode{92400}
    \country{France}
}
\email{duhart@mit.edu}

\renewcommand\shortauthors{Hati et al}

\newacronym{NVS}{NVS}{Novel View Synthesis}
\newacronym{NeRF}{NeRF}{Neural Radiance Fields}
\newacronym{KD}{KD}{Knowledge Distillation}
\newacronym{MLP}{MLP}{Multi Layer Perceptron}
\newacronym{LOD}{LOD}{Level of Detail}
\newacronym{VR}{VR}{Virtual Reality}
\newacronym{AR}{AR}{Augmented Reality}
\newacronym{CG}{CG}{Computer Graphics}
\newacronym{CV}{CV}{Computer Vision}
\newacronym{SfM}{SfM}{Structure from Motion}
\newacronym{MPI}{MPI}{Multi-Plane Images}
\newacronym{SDF}{SDF}{Signed Distance Fields}
\newacronym{BBox}{BBox}{Bounding Box}

\begin{abstract}
\gls{NeRF} learn a high-quality continuous 3D implicit representation of a scene given multiple views. While the approach has gained popularity in \gls{NVS}, its vanilla implementation is not suited for real-time applications. This paper presents DistillNeRF, a data-efficient method for breeding smaller models using \gls{KD}. Smaller models naturally benefit from lower inference times but can decrease perceptual quality. DistillNeRF optimizes training time and builds a priority grid from a teacher network as a data-efficient proxy for sampling better training examples reducing the quality loss by \toDo{$X$} on PSNR, \toDo{$X$} on SSIM, and \toDo{$X$} on LPIPS. We report up to \toDo{$factorX$} faster rendering than the original NeRF model, and reduced training time by \toDo{$X$}, \toDo{$X$} for distillation, using half the memory.
\end{abstract}

\begin{teaserfigure}
    \includegraphics[width=\textwidth]{imgs/teaser.png}
    \caption{Teaser figure to sell the paper}
    \label{fig:teaser}
\end{teaserfigure}

% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
\begin{CCSXML}
    <ccs2012>
        <concept>
            <concept_id>10010147.10010371.10010396.10010401</concept_id>
            <concept_desc>Computing methodologies~Volumetric models</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
        <concept>
            <concept_id>10010147.10010178.10010224.10010245.10010254</concept_id>
            <concept_desc>Computing methodologies~Reconstruction</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
    </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Volumetric models}
\ccsdesc[500]{Computing methodologies~Reconstruction}
% End generated code

\keywords{Neural Radiance Fields, Knowledge Distillation, Data Efficiency, Novel View Synthesis, Deep Learning}

\maketitle

\input{distillnerf}

\end{document}